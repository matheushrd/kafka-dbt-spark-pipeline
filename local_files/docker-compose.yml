version: '3'

services:
  postgres:
    container_name: postgresdb
    image: postgres:latest
    restart: unless-stopped
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: mydatabase
      POSTGRES_USER: myuser
      POSTGRES_PASSWORD: mypassword
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      spark-network:
        ipv4_address: 172.18.0.2

  kafka:
    container_name: kafka
    image: bitnami/kafka:latest
    restart: unless-stopped
    ports:
      - 9093:9093
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CLIENT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_LISTENERS=CLIENT://:9092,EXTERNAL://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=CLIENT://kafka:9092,EXTERNAL://localhost:9093
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=CLIENT
      - KAFKA_CONNECT_PLUGIN_PATH=/home/kafka/pluginss
      - KAFKA_CONNECT_CONFLUENT_SUPPORT_METRICS_ENABLE=false
      - KAFKA_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR=1
      - KAFKA_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR=1
      - KAFKA_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR=1
      - KAFKA_CONNECT_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - KAFKA_CONNECT_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - KAFKA_CONNECT_INTERNAL_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - KAFKA_CONNECT_INTERNAL_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - KAFKA_CONNECT_CONFIG_STORAGE_TOPIC=my_connect_configs
      - KAFKA_CONNECT_OFFSET_STORAGE_TOPIC=my_connect_offsets
      - KAFKA_CONNECT_STATUS_STORAGE_TOPIC=my_connect_statuses
      - listeners=PLAINTEXT://localhost:9092
    depends_on:
      - zookeeper
      - spark-master
    networks:
      spark-network:
        ipv4_address: 172.18.0.3

  zookeeper:
    container_name: zookeeper
    image: bitnami/zookeeper:latest
    ports:
      - 2181:2181
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    networks:
      spark-network:
        ipv4_address: 172.18.0.4

  # kafka-ui:
  #   image: provectuslabs/kafka-ui
  #   container_name: kafka-ui
  #   depends_on:
  #     - kafka
  #     - zookeeper
  #   ports:
  #     - 8080:8080
  #   restart: always
  #   environment:
  #     - KAFKA_CLUSTERS_0_NAME=teste_damasceno
  #     - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
  #     - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181
  #     - KAFKA_UI_HOSTNAME=http://localhost:8080
  #   networks:
  #     spark-network:
  #       ipv4_address: 172.18.0.5

  debezium:
    container_name: debezium
    image: debezium/connect:1.8
    ports:
      - "8083:8083"
    environment:
      - BOOTSTRAP_SERVERS=kafka:9092
      - GROUP_ID=1
      - CONFIG_STORAGE_TOPIC=my_connect_configs
      - OFFSET_STORAGE_TOPIC=my_connect_offsets
      - STATUS_STORAGE_TOPIC=my_connect_statuses
      - KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - INTERNAL_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - INTERNAL_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONFIG_STORAGE_REPLICATION_FACTOR=1
      - OFFSET_STORAGE_REPLICATION_FACTOR=1
      - STATUS_STORAGE_REPLICATION_FACTOR=1
    depends_on:
      - kafka
      - postgres
    volumes:
      - ./debezium-config.json:/debezium/config/debezium-config.json
    networks:
      spark-network:
        ipv4_address: 172.18.0.6

  x-minio-common: &minio-common
    image: quay.io/minio/minio:RELEASE.2023-12-20T01-00-02Z
    command: server --console-address ":9001" http://minio{1...4}/data{1...2}
    expose:
      - "9000"
      - "9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      spark-network:
        ipv4_address: 172.18.0.7

  minio1:
    <<: *minio-common
    hostname: minio1
    volumes:
      - data1-1:/data1
      - data1-2:/data2
    networks:
      spark-network:
        ipv4_address: 172.18.0.8

  minio2:
    <<: *minio-common
    hostname: minio2
    volumes:
      - data2-1:/data1
      - data2-2:/data2
    networks:
      spark-network:
        ipv4_address: 172.18.0.9

  minio3:
    <<: *minio-common
    hostname: minio3
    volumes:
      - data3-1:/data1
      - data3-2:/data2
    networks:
      spark-network:
        ipv4_address: 172.18.0.10

  minio4:
    <<: *minio-common
    hostname: minio4
    volumes:
      - data4-1:/data1
      - data4-2:/data2
    networks:
      spark-network:
        ipv4_address: 172.18.0.11

  nginx:
    image: nginx:1.19.2-alpine
    hostname: nginx
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "9000:9000"
      - "9001:9001"
    depends_on:
      - minio1
      - minio2
      - minio3
      - minio4
    networks:
      spark-network:
        ipv4_address: 172.18.0.12

  spark-master:
    image: bitnami/spark:3.3
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_MASTER_WEBUI_PORT=7070
      - SPARK_MASTER_PORT=7077
    ports:
      - "7070:7070"
      - "7077:7077"
    networks:
      spark-network:
        ipv4_address: 172.18.0.13
    volumes:
      - ./spark_data:/data
    user: 'root'

  spark-worker:
    image: bitnami/spark:3.3
    container_name: spark-worker
    hostname: spark
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark-master:7077
      - SPARK_WORKER_CORES=3
      - SPARK_WORKER_MEMORY=2g
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_WORKER_PORT=7078
      - SPARK_WORKER_WEBUI_PORT=7171
      - SPARK_USER=spark
    ports:
      - "7171:7171"
    networks:
      spark-network:
        ipv4_address: 172.18.0.14

  trino-coordinator:
    container_name: trino
    image: trinodb/trino:latest
    hostname: trino-coordinator
    environment:
      - TRINO_ENVIRONMENT=production
    ports:
      - 8080:8080
    volumes:
      - $PWD/volumes/trino/etc:/etc/trino
    networks:
      spark-network:
       ipv4_address: 172.18.0.18
    # command:
    #   - /bin/bash
    #   - -c 
    #   - |
    #     find /usr/lib/trino -name alluxio*shaded* -exec rm {} \;
    #     cp /tmp/alluxio-files/alluxio-enterprise-*-client.jar /usr/lib/trino/plugin/hive/
    #     cp /tmp/alluxio-files/alluxio-enterprise-*-client.jar /usr/lib/trino/plugin/hudi/
    #     cp /tmp/alluxio-files/alluxio-enterprise-*-client.jar /usr/lib/trino/plugin/iceberg/
    #     cp /tmp/alluxio-files/alluxio-enterprise-*-client.jar /usr/lib/trino/plugin/delta-lake/
    #     cp -R /tmp/etc-trino/* /etc/trino/
    #     cp /tmp/alluxio-files/alluxio-core-site.xml /etc/trino/core-site.xml
    #     mkdir -p /home/trino && chown trino:trino /home/trino
    #     /usr/lib/trino/bin/run-trino

  # trino-worker:
  #   image: trinodb/trino:latest
  #   container_name: trino-worker
  #   hostname: trino-worker
  #   environment:
  #     - TRINO_ENVIRONMENT=production
  #     - TRINO_DISCOVERY_URI=localhost:8080
  #   volumes:
  #     - $PWD/volumes/trino/etc:/etc/trino
  #   networks:
  #      spark-network:
  #       ipv4_address: 172.18.0.19

  mariadb:
    container_name: mariadb
    hostname: mariadb
    image: mariadb:10.5.8
    ports:
      - 3306:3306
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_USER=dbuser
      - MYSQL_PASSWORD=dbpassword
      - MYSQL_DATABASE=schema_registry
    networks:
      spark-network:
       ipv4_address: 172.18.0.21

  hive-metastore:
    container_name: hive-metastore
    hostname: hive-metastore
    image: 'bitsondatadev/hive-metastore:latest'
    ports:
      - 9083:9083 # Metastore Thrift
    volumes:
      - $PWD/volumes/hive/conf/metastore-site.xml:/opt/apache-hive-metastore-3.0.0-bin/conf/metastore-site.xml:ro
    environment:
      - METASTORE_DB_HOSTNAME=mariadb
    depends_on:
      - mariadb
    networks:
      spark-network:
       ipv4_address: 172.18.0.22
    # entrypoint: >
    #   /bin/sh -c "
    #     cp /tmp/alluxio-files/alluxio-enterprise-*-client.jar /opt/apache-hive-metastore-3.0.0-bin/lib/
    #     echo '#### Hive /opt/apache-hive-metastore-3.0.0-bin/lib/ ###'
    #     ls -al /opt/apache-hive-metastore-3.0.0-bin/lib/ | grep alluxio
    #     /entrypoint.sh
    #     "
  # trinodb:
  #   image: trinodb/trino:latest
  #   container_name: trinodb
  #   ports:
  #     - 8080:8080
  #   volumes:
  #     #- ./etc:/usr/lib/trino/etc:ro
  #     - ./etc:/usr/lib/trino/plugin
  #   #   - ./trino_files/config.properties:/data/trino/etc/config.properties
  #   # #user: 'root'
  #   networks:
  #     spark-network:
  #       ipv4_address: 172.18.0.15

  # hive-metastore:
  #   hostname: hive-metastore
  #   image: 'bitsondatadev/hive-metastore:latest'
  #   ports:
  #     - '9083:9083' # Metastore Thrift
  #   volumes:
  #     - ./conf/metastore-site.xml:/opt/apache-hive-metastore-3.0.0-bin/conf/metastore-site.xml:ro
  #   environment:
  #     METASTORE_DB_HOSTNAME: mariadb
  #   networks:
  #     spark-network:
  #       ipv4_address: 172.18.0.16


networks:
  spark-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.18.0.0/24

volumes:
  postgres_data:
  data1-1:
  data1-2:
  data2-1:
  data2-2:
  data3-1:
  data3-2:
  data4-1:
  data4-2: